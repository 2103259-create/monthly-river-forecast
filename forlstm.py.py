# -*- coding: utf-8 -*-
"""Another copy of Double power (Accept) of FORLSTM.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1nK8bk4QF8DT74aX86MsIiIiSYQN0Zl-p
"""

# === 1. Imports ===
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import re
from google.colab import files


# === 2. Upload Both Files ===
print("Please upload both SG SOKOR.txt and SG Kelantan.txt")
uploaded = files.upload()

# === 3. Parser Function ===
def parse_tideda_txt(filepath):
    data = []
    with open(filepath, 'r') as f:
        lines = f.readlines()

    current_year = None
    for line in lines:
        line = line.strip()
        if line.startswith('Daily means') and 'Year' in line:
            match = re.search(r'Year\s+(\d{4})', line)
            if match:
                current_year = int(match.group(1))
        elif line and re.match(r'^\d+\s+', line) and current_year:
            parts = re.split(r'\s+', line)
            try:
                day = int(parts[0])
                for i, value in enumerate(parts[1:13]):
                    if value != '?':
                        date = pd.Timestamp(year=current_year, month=i+1, day=day)
                        flow = float(value)
                        data.append((date, flow))
            except:
                continue
    df = pd.DataFrame(data, columns=['Date', 'Flow'])
    df.set_index('Date', inplace=True)
    df.sort_index(inplace=True)
    return df

# === 4. Process both rivers ===
# === 4. Save to df_sokor and df_kelantan ===
for filename in uploaded.keys():
    df_raw = parse_tideda_txt(filename)
    df_filtered = df_raw.loc['2000-01-01':'2020-04-30']
    df_filtered['Flow'] = df_filtered['Flow'].interpolate(method='time')

    if "sokor" in filename.lower():
        df_sokor = df_filtered.copy()
    elif "kelantan" in filename.lower():
        df_kelantan = df_filtered.copy()

# Create a dictionary to store the dataframes for both rivers
rivers = {
    "Sungai Sokor": df_sokor,
    "Sungai Kelantan": df_kelantan
}

# === 5. Display stats for each river ===
for river_name, df in rivers.items():
    print(f"\n📘 {river_name}")
    print("Date Range:", df.index.min(), "→", df.index.max())
    print("Total Data Points:", len(df))
    print("Remaining Missing Values:", df['Flow'].isna().sum())
    print(df.describe())

# === 6. Plot both river flows ===
plt.figure(figsize=(16,6))
for river_name, df in rivers.items():
    plt.plot(df.index, df['Flow'], label=river_name)
plt.title('Daily River Flow: Sungai Sokor vs Sungai Kelantan')
plt.xlabel('Date')
plt.ylabel('Flow (m)')
plt.legend()
plt.grid(True)
plt.show()

"""##STEP 2: Augmented Dickey-Fuller (ADF) Test for Stationarity

ARIMA requires a stationary time series, so let’s check both rivers using the ADF test.
"""

from statsmodels.tsa.stattools import adfuller

def run_adf_test(series, river_name):
    print(f"--- ADF Test for {river_name} ---")
    result = adfuller(series.dropna())
    print(f"ADF Statistic: {result[0]:.4f}")
    print(f"p-value: {result[1]:.4f}")
    for key, value in result[4].items():
        print(f"Critical Value ({key}): {value:.4f}")
    if result[1] <= 0.05:
        print("✅ The series is stationary (reject H0)\n")
    else:
        print("❌ The series is NOT stationary (fail to reject H0)\n")

run_adf_test(df_sokor['Flow'], "Sungai Sokor")
run_adf_test(df_kelantan['Flow'], "Sungai Kelantan")

"""Both rivers are already stationary — which means:

 do not need differencing (d = 0)

 proceed directly to ARIMA (p, 0, q) model selection

Next step: find the best values of p and q using AIC/BIC

#USE PDM.ARIMA(HELP)

LSTM Model.
"""

!pip install -q tensorflow

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import mean_squared_error, mean_absolute_error
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense

"""Prepare LSTM dataset"""

def create_sequences(data, window_size):
    X, y = [], []
    for i in range(window_size, len(data)):
        X.append(data[i - window_size:i])
        y.append(data[i])
    return np.array(X), np.array(y)

"""##Process data for LSTM
Sokor
"""

# Apply log transformation
df_sokor['Flow_log'] = np.log(df_sokor['Flow'] + 1) # Add 1 to handle zero values

# Use log-transformed data
series = df_sokor['Flow_log'].values.reshape(-1, 1)

# Normalize
scaler = MinMaxScaler()
series_scaled = scaler.fit_transform(series)

# Create sequences (window of past 30 days to predict next)
window_size = 30
X, y = create_sequences(series_scaled, window_size)

# Split 80/20
split_idx = int(len(X) * 0.8)
X_train, X_test = X[:split_idx], X[split_idx:]
y_train, y_test = y[:split_idx], y[split_idx:]

# Reshape for LSTM: [samples, timesteps, features]
X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))
X_test  = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))



"""##Build and train LSTM"""

from keras.callbacks import EarlyStopping
model = Sequential()
model.add(LSTM(50, activation='relu', input_shape=(window_size, 1)))
model.add(Dense(1))
model.compile(optimizer='adam', loss='mse')
#history = model.fit(X_train, y_train, epochs=20, batch_size=32, validation_split=0.1, verbose=1)
early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)

history = model.fit(
    X_train, y_train,
    epochs=100,
    batch_size=32,
    validation_split=0.1,
    callbacks=[early_stop],
    verbose=1
)

"""##Predict and evaluate"""

# Predict
y_pred = model.predict(X_test)

# Inverse transform (first inverse scaler, then exp)
y_test_inv = np.exp(scaler.inverse_transform(y_test.reshape(-1,1))) - 1
y_pred_inv = np.exp(scaler.inverse_transform(y_pred)) - 1

# Evaluation
rmse = np.sqrt(mean_squared_error(y_test_inv, y_pred_inv))
mae = mean_absolute_error(y_test_inv, y_pred_inv)
print(f"\n📊 LSTM Evaluation - Sungai Sokor")
print(f"RMSE: {rmse:.4f}")
print(f"MAE: {mae:.4f}")

from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score, confusion_matrix

# === Regression Metrics ===
rmse = np.sqrt(mean_squared_error(y_test_inv, y_pred_inv))
mae = mean_absolute_error(y_test_inv, y_pred_inv)
r2 = r2_score(y_test_inv, y_pred_inv)

print(f"📊 LSTM Evaluation")
print(f"RMSE: {rmse:.4f}")
print(f"MAE: {mae:.4f}")
print(f"R² Score: {r2:.4f}")

"""##Plot"""

plt.figure(figsize=(16,6))
plt.plot(y_test_inv, label='Actual')
plt.plot(y_pred_inv, label='Predicted', linestyle='--')
plt.title('Sungai Sokor - LSTM Forecast vs Actual')
plt.xlabel('Days')
plt.ylabel('Flow (m)')
plt.legend()
plt.grid(True)
plt.show()

"""## To Do for Sungai Kelantan"""

# Apply log transformation
df_kelantan['Flow_log'] = np.log(df_kelantan['Flow'] + 1) # Add 1 to handle zero values

# Use log-transformed data
series = df_kelantan['Flow_log'].values.reshape(-1, 1)

# Normalize
scaler = MinMaxScaler()
series_scaled = scaler.fit_transform(series)

# Create sequences (window of past 30 days to predict next)
window_size = 30
X, y = create_sequences(series_scaled, window_size)

# Split 80/20
split_idx = int(len(X) * 0.8)
X_train, X_test = X[:split_idx], X[split_idx:]
y_train, y_test = y[:split_idx], y[split_idx:]

# Reshape for LSTM: [samples, timesteps, features]
X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))
X_test  = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))

model = Sequential()
model.add(LSTM(50, activation='relu', input_shape=(window_size, 1)))
model.add(Dense(1))
model.compile(optimizer='adam', loss='mse')
#history = model.fit(X_train, y_train, epochs=20, batch_size=32, validation_split=0.1, verbose=1)
early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)

history = model.fit(
    X_train, y_train,
    epochs=100,
    batch_size=32,
    validation_split=0.1,
    callbacks=[early_stop],
    verbose=1
)

# Predict
y_pred = model.predict(X_test)

# Inverse transform (first inverse scaler, then exp)
y_test_inv = np.exp(scaler.inverse_transform(y_test.reshape(-1,1))) - 1
y_pred_inv = np.exp(scaler.inverse_transform(y_pred)) - 1

# Evaluation
rmse = np.sqrt(mean_squared_error(y_test_inv, y_pred_inv))
mae = mean_absolute_error(y_test_inv, y_pred_inv)
print(f"\n📊 LSTM Evaluation - Sungai Kelantan")
print(f"RMSE: {rmse:.4f}")
print(f"MAE: {mae:.4f}")



plt.figure(figsize=(16,6))
plt.plot(y_test_inv, label='Actual')
plt.plot(y_pred_inv, label='Predicted', linestyle='--')
plt.title('Sungai Kelantan - LSTM Forecast vs Actual')
plt.xlabel('Days')
plt.ylabel('Flow (m)')
plt.legend()
plt.grid(True)
plt.show()



"""##With tuning"""

!pip install -q keras-tuner

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import mean_squared_error, mean_absolute_error
from keras.models import Sequential
from keras.layers import LSTM, Dense, Dropout
from keras.optimizers import Adam, RMSprop
import keras_tuner as kt

def create_sequences(data, window_size):
    X, y = [], []
    for i in range(window_size, len(data)):
        X.append(data[i-window_size:i])
        y.append(data[i])
    return np.array(X), np.array(y)

def build_model(hp):
    model = Sequential()
    model.add(LSTM(
        hp.Int('units', min_value=32, max_value=128, step=16),
        activation='relu',
        input_shape=(window_size, 1)
    ))
    model.add(Dropout(hp.Float('dropout', 0.0, 0.3, step=0.1)))
    model.add(Dense(1))

    hp_learning_rate = hp.Choice('learning_rate', [1e-2, 1e-3, 1e-4])
    optimizer = hp.Choice('optimizer', ['adam', 'rmsprop'])

    if optimizer == 'adam':
        opt = Adam(learning_rate=hp_learning_rate)
    else:
        opt = RMSprop(learning_rate=hp_learning_rate)

    model.compile(optimizer=opt, loss='mse')
    return model

def run_lstm_with_tuning(df, river_name, window_size=30, max_trials=5, save=True):
    import os
    import joblib
    from sklearn.preprocessing import MinMaxScaler
    from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, accuracy_score, recall_score, precision_score, f1_score, confusion_matrix
    import matplotlib.pyplot as plt
    import numpy as np
    import keras_tuner as kt

    print(f"\n===== Running LSTM for {river_name} =====")

    # === Preprocessing ===
    series = df['Flow_log'].values.reshape(-1, 1)
    scaler = MinMaxScaler()
    series_scaled = scaler.fit_transform(series)

    X, y = create_sequences(series_scaled, window_size)
    split = int(len(X) * 0.8)
    X_train, X_test = X[:split], X[split:]
    y_train, y_test = y[:split], y[split:]
    X_train = X_train.reshape((-1, window_size, 1))
    X_test = X_test.reshape((-1, window_size, 1))

    # === Tuning ===
    tuner = kt.RandomSearch(
        build_model,
        objective='val_loss',
        max_trials=max_trials,
        executions_per_trial=3,
        directory='lstm_tuning',
        project_name=f'{river_name.lower().replace(" ", "_")}_lstm'
    )
    tuner.search(X_train, y_train, epochs=10, validation_split=0.1, verbose=1)
    best_model = tuner.get_best_models(num_models=1)[0]

    # === Forecasting & Evaluation ===
    y_pred = best_model.predict(X_test)
    y_test_inv = np.exp(scaler.inverse_transform(y_test.reshape(-1, 1))) - 1
    y_pred_inv = np.exp(scaler.inverse_transform(y_pred)) - 1

    rmse = np.sqrt(mean_squared_error(y_test_inv, y_pred_inv))
    mae = mean_absolute_error(y_test_inv, y_pred_inv)
    r2 = r2_score(y_test_inv, y_pred_inv)

    print(f"\n📊 LSTM Evaluation - {river_name}")
    print(f"RMSE: {rmse:.4f}")
    print(f"MAE: {mae:.4f}")
    print(f"R² Score: {r2:.4f}")

    # === Classification Metrics (top 10% flows) ===
    threshold = np.percentile(y_test_inv, 90)
    y_test_bin = (y_test_inv.flatten() > threshold).astype(int)
    y_pred_bin = (y_pred_inv.flatten() > threshold).astype(int)

    accuracy = accuracy_score(y_test_bin, y_pred_bin)
    recall = recall_score(y_test_bin, y_pred_bin)
    precision = precision_score(y_test_bin, y_pred_bin)
    f1 = f1_score(y_test_bin, y_pred_bin)

    print(f"\n🔍 Classification Metrics (Threshold: {threshold:.2f} m³/s)")
    print(f"Accuracy: {accuracy:.4f}")
    print(f"Recall: {recall:.4f}")
    print(f"Precision: {precision:.4f}")
    print(f"F1-Score: {f1:.4f}")
    print("Confusion Matrix:")
    print(confusion_matrix(y_test_bin, y_pred_bin))

    # === Plots ===
    plt.figure(figsize=(16, 6))
    plt.plot(y_test_inv, label='Actual')
    plt.plot(y_pred_inv, label='Predicted', linestyle='--')
    plt.title(f"{river_name} - LSTM Forecast with Keras Tuner")
    plt.xlabel("Days")
    plt.ylabel("Flow (m³/s)")
    plt.legend()
    plt.grid(True)
    plt.show()

    # === Optional Saving ===
    if save:
        os.makedirs("models", exist_ok=True)
        best_model.save(f"models/model_{river_name.lower().replace(' ', '_')}_lstm.h5")
        joblib.dump(scaler, f"models/scaler_{river_name.lower().replace(' ', '_')}.pkl")
        print(f"\n💾 Model and scaler saved for {river_name}.")

    return best_model, scaler, y_test_inv, y_pred_inv

df_kelantan['Flow_log'] = np.log(df_kelantan['Flow'] + 1)

model_kelantan, scaler_kelantan, y_test_kelantan_inv, y_pred_kelantan_inv = run_lstm_with_tuning(
    df_kelantan,
    river_name="Sungai Kelantan",
    window_size=30,
    max_trials=5,
    save=True  # ← model + scaler will auto-save!
)

df_sokor['Flow_log'] = np.log(df_sokor['Flow'] + 1)

model_sokor, scaler_sokor, y_test_sokor_inv, y_pred_sokor_inv = run_lstm_with_tuning(
    df_sokor,
    river_name="Sungai Sokor",
    window_size=30,
    max_trials=5,
    save=True  # ← model + scaler will auto-save!
)

#Usage (replace df_sokor and df_kelantan with your own cleaned DataFrames with 'Flow_log' column)
#df_sokor['Flow_log'] = np.log(df_sokor['Flow'] + 1)

#run_lstm_with_tuning(df_sokor, "Sungai Sokor")

'''
df_kelantan['Flow_log'] = np.log(df_kelantan['Flow'] + 1)
model_kelantan, scaler_kelantan, y_test_kelantan_inv, y_pred_kelantan_inv = run_lstm_with_tuning(df_kelantan, "Sungai Kelantan")
'''

from tensorflow.keras.models import save_model
import joblib
import os

os.makedirs("models", exist_ok=True)
save_model(model_kelantan, "models/model_kelantan_lstm.keras")
joblib.dump(scaler_kelantan, "models/scaler_kelantan.pkl")

save_model(model_kelantan, "models/model_kelantan_lstm.keras")
joblib.dump(scaler_kelantan, "models/scaler_kelantan.pkl")

# Create folder if not exists
import os, joblib
import tensorflow as tf # Import tensorflow

os.makedirs("models", exist_ok=True)

# Save Kelantan model & scaler
tf.keras.models.save_model(model_kelantan, "models/model_kelantan_lstm.keras") # Use tf.keras.models.save_model
joblib.dump(scaler_kelantan, "models/scaler_kelantan.pkl")

model_kelantan = model
scaler_kelantan = scaler

os.makedirs("models", exist_ok=True)
model_kelantan.save("models/model_kelantan_lstm.h5")  # You may also use .keras
joblib.dump(scaler_kelantan, "models/scaler_kelantan.pkl")

model_sokor = model
scaler_sokor = scaler

os.makedirs("models", exist_ok=True)
model_sokor.save("models/model_sokor_lstm.keras")  # Changed to .keras
joblib.dump(scaler_sokor, "models/scaler_sokor.pkl")

# Kelantan residuals
residuals_kelantan = y_test_kelantan_inv.flatten() - y_pred_kelantan_inv.flatten()

plt.figure(figsize=(10,4))
plt.plot(residuals_kelantan)
plt.title("Kelantan Residuals Over Time")
plt.grid(True)
plt.show()
'''
# Sokor residuals
residuals_sokor = y_test_sokor_inv.flatten() - y_pred_sokor_inv.flatten()

plt.figure(figsize=(10,4))
plt.plot(residuals_sokor)
plt.title("Sokor Residuals Over Time")
plt.grid(True)
plt.show()
'''

# Naïve: yesterday = today (Kelantan)
naive_preds_kelantan = y_test_kelantan_inv[:-1]
naive_actuals_kelantan = y_test_kelantan_inv[1:]
naive_rmse_k = np.sqrt(mean_squared_error(naive_actuals_kelantan, naive_preds_kelantan))
print(f"Naïve RMSE (Kelantan): {naive_rmse_k:.2f}")

def forecast_lstm(model, scaler, input_sequence, n_steps=30):
    """
    Forecasts next n steps from the last known Flow_log values.
    Returns forecast in original scale (m³/s).
    """
    forecast = []
    input_seq = scaler.transform(input_sequence.reshape(-1,1)).flatten().tolist()

    for _ in range(n_steps):
        x_input = np.array(input_seq[-30:]).reshape(1, 30, 1)
        yhat = model.predict(x_input, verbose=0)[0][0]
        forecast.append(yhat)
        input_seq.append(yhat)

    # Inverse transform and exponentiate back to original scale
    forecast_scaled = np.array(forecast).reshape(-1,1)
    forecast_unscaled = scaler.inverse_transform(forecast_scaled).flatten()
    forecast_final = np.exp(forecast_unscaled) - 1
    return forecast_final

'''
def forecast_lstm(model, scaler, input_sequence, n_steps=30):
    """
    Generates forecast using trained LSTM model.

    Parameters:
    - model: Trained Keras model
    - scaler: Corresponding MinMaxScaler used during training
    - input_sequence: Last known values (unscaled)
    - n_steps: Forecast horizon (e.g., 7/14/30)

    Returns:
    - Forecast in original scale
    """
    forecast = []
    input_seq = scaler.transform(input_sequence.reshape(-1,1)).flatten().tolist()

    for _ in range(n_steps):
        x_input = np.array(input_seq[-30:]).reshape(1, 30, 1)
        yhat = model.predict(x_input, verbose=0)[0][0]
        forecast.append(yhat)
        input_seq.append(yhat)

    forecast_scaled = np.array(forecast).reshape(-1,1)
    forecast_unscaled = scaler.inverse_transform(forecast_scaled).flatten()
    return forecast_unscaled
    '''

from tensorflow.keras.models import load_model
import joblib

model = load_model("models/model_kelantan_lstm.keras")
scaler = joblib.load("models/scaler_kelantan.pkl")

'''
from tensorflow.keras.losses import MeanSquaredError
custom_objects = {'mse': MeanSquaredError()}
model = load_model("models/model_kelantan_lstm.h5", custom_objects=custom_objects)
'''

from tensorflow.keras.models import load_model
import joblib
import matplotlib.pyplot as plt
import numpy as np

# Load model and scaler
model = load_model("models/model_kelantan_lstm.keras")
scaler = joblib.load("models/scaler_kelantan.pkl")

# Get last 30 days of log-transformed input
last_30 = df_kelantan['Flow_log'].values[-30:]  # ✅ Still in log

# Forecast (forecast_lstm handles scaling and inverse-scaling)
n_steps = 30
forecast = forecast_lstm(model, scaler, last_30, n_steps=n_steps)  # ✅ forecast is un-logged

# Convert actual to real scale for plot
last_30_actual = np.exp(last_30) - 1  # ✅ True last 30 values

# Plot
plt.figure(figsize=(16,6))
plt.plot(np.arange(0, 30), last_30_actual, label='Last 30 Actual', color='blue')
plt.plot(np.arange(30, 60), forecast, label='Forecast', color='orange')
plt.title("30-Day Forecast - Kelantan")
plt.xlabel("Days")
plt.ylabel("Flow (m³/s)")
plt.legend()
plt.grid(True)
plt.show()

print("Forecast values:", forecast)
print("Last 30 values (exp-scaled):", np.exp(last_30) - 1)

from tensorflow.keras.models import load_model
import joblib
import matplotlib.pyplot as plt
import numpy as np

# Load model and scaler
model = load_model("models/model_sokor_lstm.keras") # Changed to .keras
scaler = joblib.load("models/scaler_sokor.pkl")

# Get last 30 days of log-transformed input
last_30 = df_sokor['Flow_log'].values[-30:]  # ✅ Still in log

# Forecast (forecast_lstm handles scaling and inverse-scaling)
n_steps = 30
forecast = forecast_lstm(model, scaler, last_30, n_steps=n_steps)  # ✅ forecast is un-logged

# Convert actual to real scale for plot
last_30_actual = np.exp(last_30) - 1  # ✅ True last 30 values

# Plot
plt.figure(figsize=(16,6))
plt.plot(np.arange(0, 30), last_30_actual, label='Last 30 Actual', color='blue')
plt.plot(np.arange(30, 60), forecast, label='Forecast', color='orange')
plt.title("30-Day Forecast - Sokor")
plt.xlabel("Days")
plt.ylabel("Flow (m³/s)")
plt.legend()
plt.grid(True)
plt.show()

print("Forecast values:", forecast)
print("Last 30 values (exp-scaled):", np.exp(last_30) - 1)

"""X-Axis Alignment	Actual ends at 29, forecast starts at 30
Inverse Transform (exp)
Applied only once to both actual and forecast values

Forecast Curve
Smooth, gradually increasing — expected behavior from LSTM

No Flat-Line Issue
Previously caused by missing np.exp(...) — now resolved

Legend	Clearly distinguishes "Last 30 Actual" vs "Forecast"

Label & Units	Y-axis properly shows Flow (m³/s)
"""



'''
from tensorflow.keras.models import load_model # Import load_model from tensorflow.keras.models
import joblib
import matplotlib.pyplot as plt
import numpy as np


# custom_objects = {'mse': MeanSquaredError()} # No longer needed
# Just load model directly — no custom_objects needed for 'mse'
model = load_model("models/model_kelantan_lstm.keras") # Use load_model from tensorflow.keras.models
scaler = joblib.load("models/scaler_kelantan.pkl")

# Get last 30 days (assume trained on 'Flow_log')
last_30 = df_kelantan['Flow_log'].values[-30:]

n_steps = 30 # Define n_steps here
forecast = forecast_lstm(model, scaler, last_30, n_steps=n_steps)

# Plot
plt.figure(figsize=(16,6))

# Plot actual (inverse transformed)
actual = np.exp(last_30) - 1
x_actual = np.arange(0, 30)
plt.plot(x_actual, actual, label='Last 30 Actual (Inverse Transformed)', color='orange')

# Plot forecast immediately after
x_forecast = np.arange(30, 60)
plt.plot(x_forecast, forecast, label='Forecast', color='blue')

plt.title("30-Day Forecast - Kelantan")
plt.xlabel("Days")
plt.ylabel("Flow (m³/s)")
plt.legend()
plt.grid(True)
plt.show()

# Convert last_30 from log back to original scale
last_30_actual = np.exp(last_30) - 1  # ✅ One-time transform here

# Plot
plt.figure(figsize=(16,6))
plt.plot(np.arange(len(last_30)), last_30_actual, label='Last 30 Actual')  # ✅ No np.exp here
plt.plot(np.arange(len(last_30), len(last_30) + n_steps), forecast, label='Forecast')
plt.title("30-Day Forecast - Kelantan")
plt.xlabel("Days")
plt.ylabel("Flow (m³/s)")
plt.legend()
plt.grid(True)
plt.show()
'''

print("Forecast values:", forecast)
print("Last 30 values (exp-scaled):", np.exp(last_30) - 1)

''''
# Load trained model + scaler
from tensorflow.keras.models import load_model
from tensorflow.keras.losses import mean_squared_error # Import mse

# Define custom objects dictionary
custom_objects = {"mse": mean_squared_error}

model = load_model("models/model_kelantan_lstm.h5", custom_objects=custom_objects)
scaler = joblib.load("models/scaler_kelantan.pkl")

# Get last 30 days from full series
# Make sure to use the correct column name for the series (Flow_log or Flow depending on what the model was trained on)
# Based on the previous code, it seems the model was trained on 'Flow_log'
last_30 = df_kelantan['Flow_log'].values[-30:]

forecast = forecast_lstm(model, scaler, last_30, n_steps=30)

# Plot forecast
plt.figure(figsize=(16,6))
plt.plot(np.arange(len(last_30), len(last_30) + n_steps), forecast, label='Forecast')
plt.plot(np.arange(len(last_30)), np.exp(last_30)-1, label='Last 30 Actual (Inverse Transformed)') # Plot actuals for context
plt.title("30-Day Forecast - Kelantan")
plt.xlabel("Days")
plt.ylabel("Flow (m)")
plt.legend()
plt.grid(True)
plt.show()
'''

"""# Task
Explain how to deploy the developed LSTM models for river level forecasting on Streamlit, including saving the necessary components, structuring the Streamlit application, loading components, implementing file upload, preprocessing uploaded data, implementing forecasting logic, displaying results, adding user interface elements, creating a requirements file, and running the Streamlit application.

## Save necessary components

### Subtask:
Ensure the trained LSTM models and the scalers for both rivers (`model_kelantan_lstm.keras`, `scaler_kelantan.pkl`, `model_sokor_lstm.keras`, `scaler_sokor.pkl`) are saved in a directory that can be accessed by your Streamlit application.

**Reasoning**:
Check if the required model and scaler files exist in the `/content/models/` directory.
"""

import os

required_files = [
    "model_kelantan_lstm.keras",
    "scaler_kelantan.pkl",
    "model_sokor_lstm.keras",
    "scaler_sokor.pkl"
]

models_dir = "/content/models/"

all_files_exist = True
for file_name in required_files:
    file_path = os.path.join(models_dir, file_name)
    if not os.path.exists(file_path):
        print(f"Error: Required file not found: {file_path}")
        all_files_exist = False
    else:
        print(f"Found: {file_path}")

if all_files_exist:
    print("\nAll required model and scaler files are present.")
    print(f"The models directory path is: {models_dir}")
else:
    print("\nSome required files are missing. Please re-run the relevant training and saving steps.")

"""## Structure the streamlit application

### Subtask:
Create a Python file (e.g., `app.py`) that will contain the Streamlit code.

**Reasoning**:
Create the `app.py` file with necessary imports and a basic structure.
"""

!pip install streamlit

import streamlit as st #x
import pandas as pd
import numpy as np
import tensorflow as tf
import joblib
import os

def main():
    st.title("River Flow Forecasting with LSTM")

if __name__ == "__main__":
    main()

"""**Reasoning**:
The previous command failed because the `streamlit` library is not installed in the current environment. Install `streamlit` using pip.

**Reasoning**:
Now that `streamlit` is installed, create the `app.py` file with the necessary imports and a basic structure as requested in the subtask instructions.
"""

# %%writefile app.py
import streamlit as st
import pandas as pd
import numpy as np
import tensorflow as tf
import joblib
import os

def main():
    st.title("River Flow Forecasting with LSTM")
    # Placeholder for file upload
    uploaded_file = st.file_uploader("Upload your river flow data (TXT format)", type=["txt"])

    if uploaded_file is not None:
        # Placeholder for processing and forecasting logic
        st.write("File uploaded successfully. Processing data...")

if __name__ == "__main__":
    main()

"""## Load components in streamlit

### Subtask:
In the Streamlit app, load the saved models and scalers.

**Reasoning**:
Modify the `app.py` file to load the saved Keras models and MinMaxScaler scalers for both Kelantan and Sokor rivers.
"""

# %%writefile app.py
import streamlit as st
import pandas as pd
import numpy as np
import tensorflow as tf
import joblib
import os
import re

# === Configuration ===
MODELS_DIR = "models" # Directory where models and scalers are saved
WINDOW_SIZE = 30 # Must match the window size used during training

# === Helper function to parse the data ===
def parse_tideda_txt(filepath):
    data = []
    # In Streamlit, filepath is a file-like object
    lines = filepath.readlines()

    current_year = None
    # Decode lines if they are bytes
    lines = [line.decode('utf-8') if isinstance(line, bytes) else line for line in lines]

    for line in lines:
        line = line.strip()
        if line.startswith('Daily means') and 'Year' in line:
            match = re.search(r'Year\s+(\d{4})', line)
            if match:
                current_year = int(match.group(1))
        elif line and re.match(r'^\d+\s+', line) and current_year:
            parts = re.split(r'\s+', line)
            try:
                day = int(parts[0])
                for i, value in enumerate(parts[1:13]):
                    if value != '?':
                        # Construct date string and parse
                        date_str = f"{current_year}-{i+1:02d}-{day:02d}"
                        try:
                            date = pd.to_datetime(date_str)
                            flow = float(value)
                            data.append((date, flow))
                        except ValueError:
                            # Handle cases where date might be invalid (e.g., day 31 in Feb)
                            continue # Skip invalid dates
            except:
                continue
    df = pd.DataFrame(data, columns=['Date', 'Flow'])
    if not df.empty:
        df.set_index('Date', inplace=True)
        df.sort_index(inplace=True)
    return df


# === Load Models and Scalers ===
@st.cache_resource # Cache the models and scalers
def load_all_components():
    try:
        model_kelantan = tf.keras.models.load_model(os.path.join(MODELS_DIR, "model_kelantan_lstm.keras"))
        scaler_kelantan = joblib.load(os.path.join(MODELS_DIR, "scaler_kelantan.pkl"))
        model_sokor = tf.keras.models.load_model(os.path.join(MODELS_DIR, "model_sokor_lstm.keras"))
        scaler_sokor = joblib.load(os.path.join(MODELS_DIR, "scaler_sokor.pkl"))
        st.success("Models and scalers loaded successfully!")
        return model_kelantan, scaler_kelantan, model_sokor, scaler_sokor
    except Exception as e:
        st.error(f"Error loading models or scalers: {e}")
        return None, None, None, None

# === Forecasting function ===
def forecast_lstm(model, scaler, input_sequence, n_steps=30):
    """
    Forecasts next n steps from the last known Flow_log values.
    Returns forecast in original scale (m³/s).
    """
    forecast = []
    # Ensure input sequence is log-transformed and scaled
    input_scaled = scaler.transform(np.log(input_sequence.reshape(-1, 1) + 1)).flatten().tolist()

    for _ in range(n_steps):
        x_input = np.array(input_scaled[-WINDOW_SIZE:]).reshape(1, WINDOW_SIZE, 1)
        yhat = model.predict(x_input, verbose=0)[0][0]
        forecast.append(yhat)
        input_scaled.append(yhat) # Append the prediction to the input sequence for the next step

    # Inverse transform and exponentiate back to original scale
    forecast_scaled = np.array(forecast).reshape(-1,1)
    forecast_unscaled = scaler.inverse_transform(forecast_scaled).flatten()
    forecast_final = np.exp(forecast_unscaled) - 1
    return forecast_final


def main():
    st.title("River Flow Forecasting with LSTM")

    # Load models and scalers
    model_kelantan, scaler_kelantan, model_sokor, scaler_sokor = load_all_components()

    if model_kelantan is None or scaler_kelantan is None or model_sokor is None or scaler_sokor is None:
        st.stop() # Stop if models didn't load

    uploaded_file = st.file_uploader("Upload your river flow data (TXT format)", type=["txt"])

    if uploaded_file is not None:
        st.write("File uploaded successfully. Processing data...")

        # Determine river name from filename
        file_name = uploaded_file.name.lower()
        river_name = None
        model = None
        scaler = None

        if "kelantan" in file_name:
            river_name = "Sungai Kelantan"
            model = model_kelantan
            scaler = scaler_kelantan
        elif "sokor" in file_name:
            river_name = "Sungai Sokor"
            model = model_sokor
            scaler = scaler_sokor
        else:
            st.error("Could not determine river from filename. Please upload 'SG Kelantan.txt' or 'SG Sokor.txt'.")
            return

        st.write(f"Detected river: {river_name}")

        # Parse the uploaded file
        df_raw = parse_tideda_txt(uploaded_file)

        if df_raw.empty:
            st.error("Could not parse data from the uploaded file.")
            return

        st.write("Data parsed successfully. Displaying first 5 rows:")
        st.dataframe(df_raw.head())

        # Preprocess data for forecasting (get the last WINDOW_SIZE values)
        if len(df_raw) < WINDOW_SIZE:
            st.error(f"Uploaded data must contain at least {WINDOW_SIZE} data points for forecasting.")
            return

        last_sequence = df_raw['Flow'].values[-WINDOW_SIZE:]

        # Perform forecast
        st.write(f"Generating {WINDOW_SIZE}-day forecast for {river_name}...")
        forecast_values = forecast_lstm(model, scaler, last_sequence, n_steps=WINDOW_SIZE)

        st.write("Forecast generated successfully.")

        # Display results
        st.subheader(f"{WINDOW_SIZE}-Day River Flow Forecast for {river_name}")

        # Create date index for the forecast
        last_date = df_raw.index[-1]
        forecast_dates = pd.date_range(start=last_date + pd.Timedelta(days=1), periods=WINDOW_SIZE, freq='D')
        forecast_df = pd.DataFrame({'Flow (m³/s)': forecast_values}, index=forecast_dates)

        st.dataframe(forecast_df)

        # Plot actual and forecast
        plt.figure(figsize=(16, 6))
        # Plot last WINDOW_SIZE actual values
        plt.plot(df_raw.index[-WINDOW_SIZE:], df_raw['Flow'].values[-WINDOW_SIZE:], label='Last Actual', color='blue')
        # Plot forecast values
        plt.plot(forecast_df.index, forecast_df['Flow (m³/s)'], label='Forecast', color='orange', linestyle='--')

        plt.title(f"{river_name} - {WINDOW_SIZE}-Day River Flow Forecast")
        plt.xlabel("Date")
        plt.ylabel("Flow (m³/s)")
        plt.legend()
        plt.grid(True)
        st.pyplot(plt)


if __name__ == "__main__":
    main()

"""## Implement file upload

### Subtask:
Allow users to upload the new data files (`SG SOKOR.txt` and `SG Kelantan.txt`) through the Streamlit interface.

## Preprocess uploaded data

### Subtask:
Adapt the data parsing and preprocessing steps from your notebook to handle the uploaded files within the Streamlit app. This includes applying the log transformation and scaling.

**Reasoning**:
Adapt the data parsing and preprocessing steps from the notebook to handle the uploaded files within the Streamlit app, including applying the log transformation and scaling. This involves calling the `parse_tideda_txt` function, checking for empty data, displaying the head, checking for sufficient data points, and extracting the last sequence for forecasting.
"""

# %%writefile app.py
import streamlit as st
import pandas as pd
import numpy as np
import tensorflow as tf
import joblib
import os
import re
import matplotlib.pyplot as plt

# === Configuration ===
MODELS_DIR = "models" # Directory where models and scalers are saved
WINDOW_SIZE = 30 # Must match the window size used during training

# === Helper function to parse the data ===
def parse_tideda_txt(filepath):
    data = []
    # In Streamlit, filepath is a file-like object
    # Use seek(0) to ensure reading from the beginning if it's already been read
    filepath.seek(0)
    lines = filepath.readlines()

    current_year = None
    # Decode lines if they are bytes
    lines = [line.decode('utf-8') if isinstance(line, bytes) else line for line in lines]

    for line in lines:
        line = line.strip()
        if line.startswith('Daily means') and 'Year' in line:
            match = re.search(r'Year\s+(\d{4})', line)
            if match:
                current_year = int(match.group(1))
        elif line and re.match(r'^\d+\s+', line) and current_year:
            parts = re.split(r'\s+', line)
            try:
                day = int(parts[0])
                for i, value in enumerate(parts[1:13]):
                    if value != '?':
                        # Construct date string and parse
                        date_str = f"{current_year}-{i+1:02d}-{day:02d}"
                        try:
                            date = pd.to_datetime(date_str)
                            flow = float(value)
                            data.append((date, flow))
                        except ValueError:
                            # Handle cases where date might be invalid (e.g., day 31 in Feb)
                            continue # Skip invalid dates
            except:
                continue
    df = pd.DataFrame(data, columns=['Date', 'Flow'])
    if not df.empty:
        df.set_index('Date', inplace=True)
        df.sort_index(inplace=True)
    return df


# === Load Models and Scalers ===
@st.cache_resource # Cache the models and scalers
def load_all_components():
    try:
        model_kelantan = tf.keras.models.load_model(os.path.join(MODELS_DIR, "model_kelantan_lstm.keras"))
        scaler_kelantan = joblib.load(os.path.join(MODELS_DIR, "scaler_kelantan.pkl"))
        model_sokor = tf.keras.models.load_model(os.path.join(MODELS_DIR, "model_sokor_lstm.keras"))
        scaler_sokor = joblib.load(os.path.join(MODELS_DIR, "scaler_sokor.pkl"))
        st.success("Models and scalers loaded successfully!")
        return model_kelantan, scaler_kelantan, model_sokor, scaler_sokor
    except Exception as e:
        st.error(f"Error loading models or scalers: {e}")
        return None, None, None, None

# === Forecasting function ===
def forecast_lstm(model, scaler, input_sequence, n_steps=30):
    """
    Forecasts next n steps from the last known Flow_log values.
    Returns forecast in original scale (m³/s).
    """
    forecast = []
    # Ensure input sequence is log-transformed and scaled
    # The input_sequence here is expected to be the original flow values (not log-transformed)
    input_scaled = scaler.transform(np.log(input_sequence.reshape(-1, 1) + 1)).flatten().tolist()

    for _ in range(n_steps):
        # Use the last WINDOW_SIZE values from the growing input_scaled list
        x_input = np.array(input_scaled[-WINDOW_SIZE:]).reshape(1, WINDOW_SIZE, 1)
        yhat = model.predict(x_input, verbose=0)[0][0]
        forecast.append(yhat)
        input_scaled.append(yhat) # Append the prediction (in scaled log space) to the input sequence for the next step

    # Inverse transform and exponentiate back to original scale
    forecast_scaled = np.array(forecast).reshape(-1,1)
    forecast_unscaled = scaler.inverse_transform(forecast_scaled).flatten()
    forecast_final = np.exp(forecast_unscaled) - 1
    return forecast_final


def main():
    st.title("River Flow Forecasting with LSTM")

    # Load models and scalers
    model_kelantan, scaler_kelantan, model_sokor, scaler_sokor = load_all_components()

    if model_kelantan is None or scaler_kelantan is None or model_sokor is None or scaler_sokor is None:
        st.stop() # Stop if models didn't load

    uploaded_file = st.file_uploader("Upload your river flow data (TXT format)", type=["txt"])

    if uploaded_file is not None:
        st.write("File uploaded successfully. Processing data...")

        # Determine river name from filename
        file_name = uploaded_file.name.lower()
        river_name = None
        model = None
        scaler = None

        if "kelantan" in file_name:
            river_name = "Sungai Kelantan"
            model = model_kelantan
            scaler = scaler_kelantan
        elif "sokor" in file_name:
            river_name = "Sungai Sokor"
            model = model_sokor
            scaler = scaler_sokor
        else:
            st.error("Could not determine river from filename. Please upload 'SG Kelantan.txt' or 'SG Sokor.txt'.")
            return

        st.write(f"Detected river: {river_name}")

        # Parse the uploaded file
        df_raw = parse_tideda_txt(uploaded_file)

        # 1. Check if the resulting DataFrame is empty after parsing.
        if df_raw.empty:
            st.error("Could not parse data from the uploaded file or the file is empty.")
            return

        st.write("Data parsed successfully. Displaying first 5 rows:")
        # 2. Display the head of the parsed DataFrame
        st.dataframe(df_raw.head())

        # 3. Check if the length of the parsed DataFrame is less than the defined WINDOW_SIZE.
        if len(df_raw) < WINDOW_SIZE:
            st.error(f"Uploaded data must contain at least {WINDOW_SIZE} data points for forecasting. Found {len(df_raw)}.")
            return

        # 4. Extract the last WINDOW_SIZE flow values
        last_sequence = df_raw['Flow'].values[-WINDOW_SIZE:]

        # Perform forecast
        st.write(f"Generating {WINDOW_SIZE}-day forecast for {river_name}...")
        forecast_values = forecast_lstm(model, scaler, last_sequence, n_steps=WINDOW_SIZE)

        st.write("Forecast generated successfully.")

        # Display results
        st.subheader(f"{WINDOW_SIZE}-Day River Flow Forecast for {river_name}")

        # Create date index for the forecast
        last_date = df_raw.index[-1]
        forecast_dates = pd.date_range(start=last_date + pd.Timedelta(days=1), periods=WINDOW_SIZE, freq='D')
        forecast_df = pd.DataFrame({'Flow (m³/s)': forecast_values}, index=forecast_dates)

        st.dataframe(forecast_df)

        # Plot actual and forecast
        plt.figure(figsize=(16, 6))
        # Plot last WINDOW_SIZE actual values
        plt.plot(df_raw.index[-WINDOW_SIZE:], df_raw['Flow'].values[-WINDOW_SIZE:], label='Last Actual', color='blue')
        # Plot forecast values
        plt.plot(forecast_df.index, forecast_df['Flow (m³/s)'], label='Forecast', color='orange', linestyle='--')

        plt.title(f"{river_name} - {WINDOW_SIZE}-Day River Flow Forecast")
        plt.xlabel("Date")
        plt.ylabel("Flow (m³/s)")
        plt.legend()
        plt.grid(True)
        st.pyplot(plt)


if __name__ == "__main__":
    main()

"""**Reasoning**:
The data parsing and initial preprocessing steps have been added to `app.py`. The next step is to implement the forecasting logic using the loaded model and scaler and display the results in the Streamlit app, including creating a DataFrame for the forecast and plotting the actual and predicted values.


"""

# %%writefile app.py
import streamlit as st
import pandas as pd
import numpy as np
import tensorflow as tf
import joblib
import os
import re
import matplotlib.pyplot as plt

# === Configuration ===
MODELS_DIR = "models" # Directory where models and scalers are saved
WINDOW_SIZE = 30 # Must match the window size used during training

# === Helper function to parse the data ===
def parse_tideda_txt(filepath):
    data = []
    # In Streamlit, filepath is a file-like object
    # Use seek(0) to ensure reading from the beginning if it's already been read
    filepath.seek(0)
    lines = filepath.readlines()

    current_year = None
    # Decode lines if they are bytes
    lines = [line.decode('utf-8') if isinstance(line, bytes) else line for line in lines]

    for line in lines:
        line = line.strip()
        if line.startswith('Daily means') and 'Year' in line:
            match = re.search(r'Year\s+(\d{4})', line)
            if match:
                current_year = int(match.group(1))
        elif line and re.match(r'^\d+\s+', line) and current_year:
            parts = re.split(r'\s+', line)
            try:
                day = int(parts[0])
                for i, value in enumerate(parts[1:13]):
                    if value != '?':
                        # Construct date string and parse
                        date_str = f"{current_year}-{i+1:02d}-{day:02d}"
                        try:
                            date = pd.to_datetime(date_str)
                            flow = float(value)
                            data.append((date, flow))
                        except ValueError:
                            # Handle cases where date might be invalid (e.g., day 31 in Feb)
                            continue # Skip invalid dates
            except:
                continue
    df = pd.DataFrame(data, columns=['Date', 'Flow'])
    if not df.empty:
        df.set_index('Date', inplace=True)
        df.sort_index(inplace=True)
    return df


# === Load Models and Scalers ===
@st.cache_resource # Cache the models and scalers
def load_all_components():
    try:
        model_kelantan = tf.keras.models.load_model(os.path.join(MODELS_DIR, "model_kelantan_lstm.keras"))
        scaler_kelantan = joblib.load(os.path.join(MODELS_DIR, "scaler_kelantan.pkl"))
        model_sokor = tf.keras.models.load_model(os.path.join(MODELS_DIR, "model_sokor_lstm.keras"))
        scaler_sokor = joblib.load(os.path.join(MODELS_DIR, "scaler_sokor.pkl"))
        st.success("Models and scalers loaded successfully!")
        return model_kelantan, scaler_kelantan, model_sokor, scaler_sokor
    except Exception as e:
        st.error(f"Error loading models or scalers: {e}")
        return None, None, None, None

# === Forecasting function ===
def forecast_lstm(model, scaler, input_sequence, n_steps=30):
    """
    Forecasts next n steps from the last known Flow_log values.
    Returns forecast in original scale (m³/s).
    """
    forecast = []
    # Ensure input sequence is log-transformed and scaled
    # The input_sequence here is expected to be the original flow values (not log-transformed)
    input_scaled = scaler.transform(np.log(input_sequence.reshape(-1, 1) + 1)).flatten().tolist()

    for _ in range(n_steps):
        # Use the last WINDOW_SIZE values from the growing input_scaled list
        x_input = np.array(input_scaled[-WINDOW_SIZE:]).reshape(1, WINDOW_SIZE, 1)
        yhat = model.predict(x_input, verbose=0)[0][0]
        forecast.append(yhat)
        input_scaled.append(yhat) # Append the prediction (in scaled log space) to the input sequence for the next step

    # Inverse transform and exponentiate back to original scale
    forecast_scaled = np.array(forecast).reshape(-1,1)
    forecast_unscaled = scaler.inverse_transform(forecast_scaled).flatten()
    forecast_final = np.exp(forecast_unscaled) - 1
    return forecast_final


def main():
    st.title("River Flow Forecasting with LSTM")

    # Load models and scalers
    model_kelantan, scaler_kelantan, model_sokor, scaler_sokor = load_all_components()

    if model_kelantan is None or scaler_kelantan is None or model_sokor is None or scaler_sokor is None:
        st.stop() # Stop if models didn't load

    uploaded_file = st.file_uploader("Upload your river flow data (TXT format)", type=["txt"])

    if uploaded_file is not None:
        st.write("File uploaded successfully. Processing data...")

        # Determine river name from filename
        file_name = uploaded_file.name.lower()
        river_name = None
        model = None
        scaler = None

        if "kelantan" in file_name:
            river_name = "Sungai Kelantan"
            model = model_kelantan
            scaler = scaler_kelantan
        elif "sokor" in file_name:
            river_name = "Sungai Sokor"
            model = model_sokor
            scaler = scaler_sokor
        else:
            st.error("Could not determine river from filename. Please upload 'SG Kelantan.txt' or 'SG Sokor.txt'.")
            return

        st.write(f"Detected river: {river_name}")

        # Parse the uploaded file
        df_raw = parse_tideda_txt(uploaded_file)

        # Check if the resulting DataFrame is empty after parsing.
        if df_raw.empty:
            st.error("Could not parse data from the uploaded file or the file is empty.")
            return

        st.write("Data parsed successfully. Displaying first 5 rows:")
        # Display the head of the parsed DataFrame
        st.dataframe(df_raw.head())

        # Check if the length of the parsed DataFrame is less than the defined WINDOW_SIZE.
        if len(df_raw) < WINDOW_SIZE:
            st.error(f"Uploaded data must contain at least {WINDOW_SIZE} data points for forecasting. Found {len(df_raw)}.")
            return

        # Extract the last WINDOW_SIZE flow values
        last_sequence = df_raw['Flow'].values[-WINDOW_SIZE:]

        # Perform forecast
        st.write(f"Generating {WINDOW_SIZE}-day forecast for {river_name}...")
        forecast_values = forecast_lstm(model, scaler, last_sequence, n_steps=WINDOW_SIZE)

        st.write("Forecast generated successfully.")

        # Display results
        st.subheader(f"{WINDOW_SIZE}-Day River Flow Forecast for {river_name}")

        # Create date index for the forecast
        last_date = df_raw.index[-1]
        forecast_dates = pd.date_range(start=last_date + pd.Timedelta(days=1), periods=WINDOW_SIZE, freq='D')
        forecast_df = pd.DataFrame({'Flow (m³/s)': forecast_values}, index=forecast_dates)

        st.dataframe(forecast_df)

        # Plot actual and forecast
        plt.figure(figsize=(16, 6))
        # Plot last WINDOW_SIZE actual values
        plt.plot(df_raw.index[-WINDOW_SIZE:], df_raw['Flow'].values[-WINDOW_SIZE:], label='Last Actual', color='blue')
        # Plot forecast values
        plt.plot(forecast_df.index, forecast_df['Flow (m³/s)'], label='Forecast', color='orange', linestyle='--')

        plt.title(f"{river_name} - {WINDOW_SIZE}-Day River Flow Forecast")
        plt.xlabel("Date")
        plt.ylabel("Flow (m³/s)")
        plt.legend()
        plt.grid(True)
        st.pyplot(plt)


if __name__ == "__main__":
    main()

"""## Implement forecasting logic

### Subtask:
Use the loaded models and scalers to generate forecasts based on the preprocessed uploaded data. This will involve adapting the `create_sequences` and `forecast_lstm` functions.

**Reasoning**:
Review the existing code in `app.py` to confirm that the loaded models and scalers are correctly used to generate forecasts based on the preprocessed uploaded data, specifically checking the `create_sequences` adaptation within `forecast_lstm`, the log transformation and scaling, the use of the loaded components, the inverse transformation of the output, and the storage of the forecast results.
"""

# %%writefile app.py
import streamlit as st
import pandas as pd
import numpy as np
import tensorflow as tf
import joblib
import os
import re
import matplotlib.pyplot as plt

# === Configuration ===
MODELS_DIR = "models" # Directory where models and scalers are saved
WINDOW_SIZE = 30 # Must match the window size used during training

# === Helper function to parse the data ===
def parse_tideda_txt(filepath):
    data = []
    # In Streamlit, filepath is a file-like object
    # Use seek(0) to ensure reading from the beginning if it's already been read
    filepath.seek(0)
    lines = filepath.readlines()

    current_year = None
    # Decode lines if they are bytes
    lines = [line.decode('utf-8') if isinstance(line, bytes) else line for line in lines]

    for line in lines:
        line = line.strip()
        if line.startswith('Daily means') and 'Year' in line:
            match = re.search(r'Year\s+(\d{4})', line)
            if match:
                current_year = int(match.group(1))
        elif line and re.match(r'^\d+\s+', line) and current_year:
            parts = re.split(r'\s+', line)
            try:
                day = int(parts[0])
                for i, value in enumerate(parts[1:13]):
                    if value != '?':
                        # Construct date string and parse
                        date_str = f"{current_year}-{i+1:02d}-{day:02d}"
                        try:
                            date = pd.to_datetime(date_str)
                            flow = float(value)
                            data.append((date, flow))
                        except ValueError:
                            # Handle cases where date might be invalid (e.g., day 31 in Feb)
                            continue # Skip invalid dates
            except:
                continue
    df = pd.DataFrame(data, columns=['Date', 'Flow'])
    if not df.empty:
        df.set_index('Date', inplace=True)
        df.sort_index(inplace=True)
    return df


# === Load Models and Scalers ===
@st.cache_resource # Cache the models and scalers
def load_all_components():
    try:
        model_kelantan = tf.keras.models.load_model(os.path.join(MODELS_DIR, "model_kelantan_lstm.keras"))
        scaler_kelantan = joblib.load(os.path.join(MODELS_DIR, "scaler_kelantan.pkl"))
        model_sokor = tf.keras.models.load_model(os.path.join(MODELS_DIR, "model_sokor_lstm.keras"))
        scaler_sokor = joblib.load(os.path.join(MODELS_DIR, "scaler_sokor.pkl"))
        st.success("Models and scalers loaded successfully!")
        return model_kelantan, scaler_kelantan, model_sokor, scaler_sokor
    except Exception as e:
        st.error(f"Error loading models or scalers: {e}")
        return None, None, None, None

# === Forecasting function ===
def forecast_lstm(model, scaler, input_sequence, n_steps=30):
    """
    Forecasts next n steps from the last known Flow_log values.
    Returns forecast in original scale (m³/s).
    """
    forecast = []
    # Ensure input sequence is log-transformed and scaled
    # The input_sequence here is expected to be the original flow values (not log-transformed)
    input_scaled = scaler.transform(np.log(input_sequence.reshape(-1, 1) + 1)).flatten().tolist()

    for _ in range(n_steps):
        # Use the last WINDOW_SIZE values from the growing input_scaled list
        x_input = np.array(input_scaled[-WINDOW_SIZE:]).reshape(1, WINDOW_SIZE, 1)
        yhat = model.predict(x_input, verbose=0)[0][0]
        forecast.append(yhat)
        input_scaled.append(yhat) # Append the prediction (in scaled log space) to the input sequence for the next step

    # Inverse transform and exponentiate back to original scale
    forecast_scaled = np.array(forecast).reshape(-1,1)
    forecast_unscaled = scaler.inverse_transform(forecast_scaled).flatten()
    forecast_final = np.exp(forecast_unscaled) - 1
    return forecast_final


def main():
    st.title("River Flow Forecasting with LSTM")

    # Load models and scalers
    model_kelantan, scaler_kelantan, model_sokor, scaler_sokor = load_all_components()

    if model_kelantan is None or scaler_kelantan is None or model_sokor is None or scaler_sokor is None:
        st.stop() # Stop if models didn't load

    uploaded_file = st.file_uploader("Upload your river flow data (TXT format)", type=["txt"])

    if uploaded_file is not None:
        st.write("File uploaded successfully. Processing data...")

        # Determine river name from filename
        file_name = uploaded_file.name.lower()
        river_name = None
        model = None
        scaler = None

        if "kelantan" in file_name:
            river_name = "Sungai Kelantan"
            model = model_kelantan
            scaler = scaler_kelantan
        elif "sokor" in file_name:
            river_name = "Sungai Sokor"
            model = model_sokor
            scaler = scaler_sokor
        else:
            st.error("Could not determine river from filename. Please upload 'SG Kelantan.txt' or 'SG Sokor.txt'.")
            return

        st.write(f"Detected river: {river_name}")

        # Parse the uploaded file
        df_raw = parse_tideda_txt(uploaded_file)

        # Check if the resulting DataFrame is empty after parsing.
        if df_raw.empty:
            st.error("Could not parse data from the uploaded file or the file is empty.")
            return

        st.write("Data parsed successfully. Displaying first 5 rows:")
        # Display the head of the parsed DataFrame
        st.dataframe(df_raw.head())

        # Check if the length of the parsed DataFrame is less than the defined WINDOW_SIZE.
        if len(df_raw) < WINDOW_SIZE:
            st.error(f"Uploaded data must contain at least {WINDOW_SIZE} data points for forecasting. Found {len(df_raw)}.")
            return

        # Extract the last WINDOW_SIZE flow values
        # This is the preprocessed data for forecasting
        last_sequence = df_raw['Flow'].values[-WINDOW_SIZE:]

        # Perform forecast
        st.write(f"Generating {WINDOW_SIZE}-day forecast for {river_name}...")
        # Call forecast_lstm with the loaded model, scaler, and the last_sequence
        forecast_values = forecast_lstm(model, scaler, last_sequence, n_steps=WINDOW_SIZE)

        st.write("Forecast generated successfully.")

        # Display results
        st.subheader(f"{WINDOW_SIZE}-Day River Flow Forecast for {river_name}")

        # Create date index for the forecast
        last_date = df_raw.index[-1]
        forecast_dates = pd.date_range(start=last_date + pd.Timedelta(days=1), periods=WINDOW_SIZE, freq='D')
        forecast_df = pd.DataFrame({'Flow (m³/s)': forecast_values}, index=forecast_dates)

        st.dataframe(forecast_df)

        # Plot actual and forecast
        plt.figure(figsize=(16, 6))
        # Plot last WINDOW_SIZE actual values
        plt.plot(df_raw.index[-WINDOW_SIZE:], df_raw['Flow'].values[-WINDOW_SIZE:], label='Last Actual', color='blue')
        # Plot forecast values
        plt.plot(forecast_df.index, forecast_df['Flow (m³/s)'], label='Forecast', color='orange', linestyle='--')

        plt.title(f"{river_name} - {WINDOW_SIZE}-Day River Flow Forecast")
        plt.xlabel("Date")
        plt.ylabel("Flow (m³/s)")
        plt.legend()
        plt.grid(True)
        st.pyplot(plt)


if __name__ == "__main__":
    main()

"""## Display results

### Subtask:
Present the forecast results to the user in the Streamlit app, potentially using plots and tables.

## Add user interface elements

### Subtask:
Include necessary Streamlit widgets for a better user experience, such as titles, headers, input fields, and buttons.

**Reasoning**:
Add Streamlit widgets for better user experience, including titles, headers, input fields, buttons, and informative messages as described in the instructions.
"""

# %%writefile app.py
import streamlit as st
import pandas as pd
import numpy as np
import tensorflow as tf
import joblib
import os
import re
import matplotlib.pyplot as plt

# === Configuration ===
MODELS_DIR = "models" # Directory where models and scalers are saved
WINDOW_SIZE = 30 # Must match the window size used during training

# === Helper function to parse the data ===
def parse_tideda_txt(filepath):
    data = []
    # In Streamlit, filepath is a file-like object
    # Use seek(0) to ensure reading from the beginning if it's already been read
    filepath.seek(0)
    lines = filepath.readlines()

    current_year = None
    # Decode lines if they are bytes
    lines = [line.decode('utf-8') if isinstance(line, bytes) else line for line in lines]

    for line in lines:
        line = line.strip()
        if line.startswith('Daily means') and 'Year' in line:
            match = re.search(r'Year\s+(\d{4})', line)
            if match:
                current_year = int(match.group(1))
        elif line and re.match(r'^\d+\s+', line) and current_year:
            parts = re.split(r'\s+', line)
            try:
                day = int(parts[0])
                for i, value in enumerate(parts[1:13]):
                    if value != '?':
                        # Construct date string and parse
                        date_str = f"{current_year}-{i+1:02d}-{day:02d}"
                        try:
                            date = pd.to_datetime(date_str)
                            flow = float(value)
                            data.append((date, flow))
                        except ValueError:
                            # Handle cases where date might be invalid (e.g., day 31 in Feb)
                            continue # Skip invalid dates
            except:
                continue
    df = pd.DataFrame(data, columns=['Date', 'Flow'])
    if not df.empty:
        df.set_index('Date', inplace=True)
        df.sort_index(inplace=True)
    return df


# === Load Models and Scalers ===
@st.cache_resource # Cache the models and scalers
def load_all_components():
    try:
        model_kelantan = tf.keras.models.load_model(os.path.join(MODELS_DIR, "model_kelantan_lstm.keras"))
        scaler_kelantan = joblib.load(os.path.join(MODELS_DIR, "scaler_kelantan.pkl"))
        model_sokor = tf.keras.models.load_model(os.path.join(MODELS_DIR, "model_sokor_lstm.keras"))
        scaler_sokor = joblib.load(os.path.join(MODELS_DIR, "scaler_sokor.pkl"))
        st.success("Models and scalers loaded successfully!")
        return model_kelantan, scaler_kelantan, model_sokor, scaler_sokor
    except Exception as e:
        st.error(f"Error loading models or scalers: {e}")
        return None, None, None, None

# === Forecasting function ===
def forecast_lstm(model, scaler, input_sequence, n_steps=30):
    """
    Forecasts next n steps from the last known Flow_log values.
    Returns forecast in original scale (m³/s).
    """
    forecast = []
    # Ensure input sequence is log-transformed and scaled
    # The input_sequence here is expected to be the original flow values (not log-transformed)
    input_scaled = scaler.transform(np.log(input_sequence.reshape(-1, 1) + 1)).flatten().tolist()

    for _ in range(n_steps):
        # Use the last WINDOW_SIZE values from the growing input_scaled list
        x_input = np.array(input_scaled[-WINDOW_SIZE:]).reshape(1, WINDOW_SIZE, 1)
        yhat = model.predict(x_input, verbose=0)[0][0]
        forecast.append(yhat)
        input_scaled.append(yhat) # Append the prediction (in scaled log space) to the input sequence for the next step

    # Inverse transform and exponentiate back to original scale
    forecast_scaled = np.array(forecast).reshape(-1,1)
    forecast_unscaled = scaler.inverse_transform(forecast_scaled).flatten()
    forecast_final = np.exp(forecast_unscaled) - 1
    return forecast_final


def main():
    # 1. Add a main title for the application
    st.title("💧 River Flow Forecasting Application")

    # 2. Add a brief introductory text
    st.write("Upload a river flow data file (.txt) to get a 30-day forecast using a trained LSTM model.")

    # Load models and scalers
    model_kelantan, scaler_kelantan, model_sokor, scaler_sokor = load_all_components()

    if model_kelantan is None or scaler_kelantan is None or model_sokor is None or scaler_sokor is None:
        st.warning("Models and scalers could not be loaded. Please ensure they are in the correct directory.")
        st.stop() # Stop if models didn't load successfully

    # 3. Include clear labels for the file uploader
    uploaded_file = st.file_uploader("Upload your river flow data (TXT format)", type=["txt"])

    if uploaded_file is not None:
        # 5. Add informative messages
        st.info("File uploaded successfully. Processing data...")

        # Determine river name from filename
        file_name = uploaded_file.name.lower()
        river_name = None
        model = None
        scaler = None

        if "kelantan" in file_name:
            river_name = "Sungai Kelantan"
            model = model_kelantan
            scaler = scaler_kelantan
        elif "sokor" in file_name:
            river_name = "Sungai Sokor"
            model = model_sokor
            scaler = scaler_sokor
        else:
            # 6. Add error messages
            st.error("Could not determine river from filename. Please upload 'SG Kelantan.txt' or 'SG Sokor.txt'.")
            return

        st.write(f"Detected river: **{river_name}**")

        # Parse the uploaded file
        df_raw = parse_tideda_txt(uploaded_file)

        # Check if the resulting DataFrame is empty after parsing.
        if df_raw.empty:
            # 6. Add error messages
            st.error("Could not parse data from the uploaded file or the file is empty.")
            return

        # 4. Add headers or subheaders for parsed data
        st.subheader("Parsed Data Preview")
        # 5. Add informative messages
        st.write("Data parsed successfully. Displaying first 5 rows:")
        # Display the head of the parsed DataFrame
        st.dataframe(df_raw.head())

        # Check if the length of the parsed DataFrame is less than the defined WINDOW_SIZE.
        if len(df_raw) < WINDOW_SIZE:
            # 6. Add error messages
            st.error(f"Uploaded data must contain at least {WINDOW_SIZE} data points for forecasting. Found only {len(df_raw)}.")
            return

        # Extract the last WINDOW_SIZE flow values
        # This is the preprocessed data for forecasting
        last_sequence = df_raw['Flow'].values[-WINDOW_SIZE:]

        # Perform forecast
        # 5. Add informative messages
        st.info(f"Generating {WINDOW_SIZE}-day forecast for {river_name}...")
        # Call forecast_lstm with the loaded model, scaler, and the last_sequence
        forecast_values = forecast_lstm(model, scaler, last_sequence, n_steps=WINDOW_SIZE)

        # 5. Add informative messages
        st.success("Forecast generated successfully.")

        # Display results
        # 4. Add headers or subheaders for forecast results
        st.subheader(f"{WINDOW_SIZE}-Day River Flow Forecast for {river_name}")

        # Create date index for the forecast
        last_date = df_raw.index[-1]
        forecast_dates = pd.date_range(start=last_date + pd.Timedelta(days=1), periods=WINDOW_SIZE, freq='D')
        forecast_df = pd.DataFrame({'Flow (m³/s)': forecast_values}, index=forecast_dates)

        st.dataframe(forecast_df)

        # Plot actual and forecast
        # 4. Add headers or subheaders for plots
        st.subheader("Forecast Visualization")
        fig, ax = plt.subplots(figsize=(16, 6))
        # Plot last WINDOW_SIZE actual values
        ax.plot(df_raw.index[-WINDOW_SIZE:], df_raw['Flow'].values[-WINDOW_SIZE:], label='Last Actual', color='blue')
        # Plot forecast values
        ax.plot(forecast_df.index, forecast_df['Flow (m³/s)'], label='Forecast', color='orange', linestyle='--')

        ax.set_title(f"{river_name} - {WINDOW_SIZE}-Day River Flow Forecast")
        ax.set_xlabel("Date")
        ax.set_ylabel("Flow (m³/s)")
        ax.legend()
        ax.grid(True)
        st.pyplot(fig)
        plt.close(fig) # Close the figure to prevent it from displaying again automatically


if __name__ == "__main__":
    main()

"""## Create a requirements file

### Subtask:
Generate a `requirements.txt` file listing all the necessary libraries for your application to run in a new environment.

**Reasoning**:
Generate the requirements.txt file using pip freeze.
"""

!pip freeze > requirements.txt

"""## Run the streamlit application

### Subtask:
Provide instructions on how to run the Streamlit application locally.

## Summary:

### Data Analysis Key Findings

*   All required model and scaler files (`model_kelantan_lstm.keras`, `scaler_kelantan.pkl`, `model_sokor_lstm.keras`, `scaler_sokor.pkl`) were successfully confirmed to be present in the `/content/models/` directory.
*   A basic Streamlit application structure was successfully created in `app.py`, including necessary imports and a file uploader placeholder, after installing the `streamlit` library.
*   The `app.py` script was updated to include a cached function (`load_all_components`) to load the Keras models and scalers, incorporating error handling for failed loading.
*   The implementation for uploading TXT files was confirmed to be present in the `app.py` script using `st.file_uploader`.
*   The data parsing logic (`parse_tideda_txt`) was integrated into the Streamlit app, along with checks for empty data and sufficient data points (`WINDOW_SIZE`) for forecasting.
*   The forecasting logic using the loaded models and scalers was successfully implemented and integrated into the `main` function of `app.py`, utilizing the last `WINDOW_SIZE` values of the uploaded data.
*   The results are displayed in the Streamlit app using a pandas DataFrame table and a Matplotlib plot comparing the last actual values with the forecast.
*   Various Streamlit widgets (`st.title`, `st.write`, `st.subheader`, `st.info`, `st.success`, `st.error`) were added to enhance the user interface with clear titles, instructions, and feedback messages.
*   A `requirements.txt` file containing the project's dependencies was successfully generated using `pip freeze`.

### Insights or Next Steps

*   The Streamlit application is now ready to be deployed and run locally, providing a user-friendly interface for river flow forecasting using trained LSTM models.
*   Further enhancements could include adding options for the user to specify the forecast horizon (number of days), potentially integrating real-time data fetching capabilities, or providing confidence intervals for the forecasts.
"""